{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import time \n",
    "import math \n",
    "import pygame\n",
    "#from keras.preprocessing import Image\n",
    "from PIL import Image\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "DISPLAYSURF=pygame.display.set_mode((500,500),0,32)\n",
    "clock=pygame.time.Clock()\n",
    "pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\",render_mode=\"rgb_array\")\n",
    "#print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.1\n",
    "Discount = 0.95\n",
    "nepisodes = 10000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "eps = 1\n",
    "\n",
    "eps_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape\n",
    "#q_table[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = state[0]/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03127143, -0.00877699, -0.03635814,  0.03389557], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "#print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.00019267773628234863\n",
      "Mean Reward: 0.019\n",
      "Episode: 1000\n",
      "Time Average: 0.0004557759761810303\n",
      "Mean Reward: 22.392\n",
      "Epsilon: 0.9753093024395111\n",
      "Episode: 2000\n",
      "Epsilon: 0.9512282354250458\n",
      "Time Average: 0.0006599957942962646\n",
      "Mean Reward: 22.8\n",
      "Epsilon: 0.9277417467531685\n",
      "Episode: 3000\n",
      "Epsilon: 0.9048351558698463\n",
      "Time Average: 0.0006038024425506592\n",
      "Mean Reward: 22.518\n",
      "Epsilon: 0.8824941446941661\n",
      "Episode: 4000\n",
      "Epsilon: 0.8607047486686201\n",
      "Time Average: 0.0005574064254760742\n",
      "Mean Reward: 21.313\n",
      "Epsilon: 0.8394533480303666\n",
      "Episode: 5000\n",
      "Epsilon: 0.818726659298009\n",
      "Time Average: 0.00038216757774353025\n",
      "Mean Reward: 21.798\n",
      "Epsilon: 0.7985117269685725\n",
      "Episode: 6000\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.000531834602355957\n",
      "Mean Reward: 21.476\n",
      "Epsilon: 0.7595669010105212\n",
      "Episode: 7000\n",
      "Epsilon: 0.7408126643807126\n",
      "Time Average: 0.0006428201198577881\n",
      "Mean Reward: 20.519\n",
      "Epsilon: 0.7225214829355084\n",
      "Episode: 8000\n",
      "Epsilon: 0.7046819235193919\n",
      "Time Average: 0.000621946096420288\n",
      "Mean Reward: 21.131\n",
      "Epsilon: 0.687282835269431\n",
      "Episode: 9000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.0005012123584747315\n",
      "Mean Reward: 19.958\n",
      "Epsilon: 0.6537628386312633\n",
      "Episode: 10000\n",
      "Epsilon: 0.6376209781063321\n",
      "Time Average: 0.0004274969100952148\n",
      "Mean Reward: 20.824\n"
     ]
    }
   ],
   "source": [
    "#pygame.init()\n",
    "  \n",
    "# CREATING CANVAS\n",
    "#canvas = pygame.display.set_mode((500, 500))\n",
    "env.reset()\n",
    "env.render()\n",
    "for episode in range (nepisodes + 1): \n",
    "    t0 = time.time()\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "    episode_reward = 0 \n",
    "\n",
    "    if episode % 1000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > eps:\n",
    "           if(-1<discrete_state[0]<30 and -1<discrete_state[1]<30 and -1<discrete_state[2]<50 and -1<discrete_state[3]<50):\n",
    "            action = np.argmax(q_table[discrete_state]) \n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) \n",
    "        new_state, reward, done, _, __ = env.step(action)\n",
    "\n",
    "        episode_reward += reward \n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 1000 == 0: \n",
    "            \n",
    "            image=env.render()\n",
    "            image=Image.fromarray(image,'RGB')\n",
    "            mode,size,data=image.mode,image.size,image.tobytes()\n",
    "            image=pygame.image.fromstring(data,size,mode)\n",
    "            DISPLAYSURF.blit(image,(0,0))\n",
    "            pygame.display.update()\n",
    "            clock.tick(100)\n",
    "\n",
    "            if done:\n",
    "             break\n",
    "\n",
    "\n",
    "\n",
    "        if not done:\n",
    "            #update q-table\n",
    "          if(-1<new_discrete_state[0]<30 and -1<new_discrete_state[1]<30 and -1<new_discrete_state[2]<50 and -1<new_discrete_state[3]<50):\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "          if(-1<discrete_state[0]<30 and -1<discrete_state[1]<30 and -1<discrete_state[2]<50 and -1<discrete_state[3]<50):\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - Learning_Rate) * current_q + Learning_Rate * (reward + Discount * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if eps > 0.05: \n",
    "        if episode_reward > prev_reward and episode > 1000:\n",
    "            eps= math.pow(eps_decay_value, episode - 1000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(eps))\n",
    "\n",
    "    t1 = time.time() \n",
    "    episode_total = t1 - t0 \n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward \n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: \n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
